<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MM-Conv</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
    </style>
</head>

<body class="bg-gray-100">

    <!-- Header Section -->
    <header class="text-center py-8 bg-white shadow-md">
        <h1 class="text-3xl font-bold">MM-Conv: A multi-modal conversational dataset for virtual humans.</h1>
        <p class="mt-2 text-gray-600">ECCV Multimodal Agents Workshop</p>
        <p class="mt-2 text-gray-700">Anna Deichler, Jim O'Regan, Jonas Beskow</p>
        <p class="mt-1 text-gray-600"><sup>1</sup>KTH Royal Institute of Technology</p>

        <div class="flex justify-center mt-4">
            <button class="bg-gray-200 text-gray-700 px-4 py-2 rounded mx-2">Paper</button>
            <button class="bg-gray-200 text-gray-700 px-4 py-2 rounded mx-2">arXiv</button>
            <button class="bg-gray-200 text-gray-700 px-4 py-2 rounded mx-2">Video</button>
            <button class="bg-gray-200 text-gray-700 px-4 py-2 rounded mx-2">Tutorial Video</button>
            <button class="bg-gray-200 text-gray-700 px-4 py-2 rounded mx-2">Code</button>
        </div>
    </header>

    <!-- Abstract Section -->
    <section class="py-8 bg-white">
        <div class="max-w-4xl mx-auto">
            <h2 class="text-2xl font-bold">Abstract</h2>
            <p class="mt-4 text-gray-700">
                In this paper, we present a novel dataset captured using a VR headset to record conversations between participants within a physics simulator (AI2-THOR). Our primary objective is to extend the field of co-speech gesture generation by incorporating rich contextual information within referential settings. Participants engaged in various conversational scenarios, all based on referential communication tasks. The dataset provides a rich set of multimodal recordings such as motion capture, speech, gaze, and scene graphs. This comprehensive dataset aims to enhance the understanding and development of gesture generation models in 3D scenes by providing diverse and contextually rich data.
            </p>
        </div>
    </section>
    <!-- Image Section -->
    <section class="py-8 bg-white">
        <div class="max-w-4xl mx-auto">
            <div class="flex justify-center">
                <img src="/media/g8119.png" alt="media/g8119.png" class="w-2/3">
            </div>
            <p class="text-center mt-4 text-lg font-semibold">Dataset visualization</p>
            <p class="text-center mt-2 text-gray-600">Description.</p>
        </div>
    </section>

    <!-- Carousel Section -->
    <section class="py-8 bg-gray-50">
        <div class="max-w-4xl mx-auto">
            <div class="flex justify-center space-x-4">
                <img src="carousel_image1.png" alt="Image 1" class="w-48 h-32 object-cover">
                <img src="carousel_image2.png" alt="Image 2" class="w-48 h-32 object-cover">
                <img src="carousel_image3.png" alt="Image 3" class="w-48 h-32 object-cover">
                <img src="carousel_image4.png" alt="Image 4" class="w-48 h-32 object-cover">
            </div>
        </div>
    </section>
<section class="py-8 bg-white">
    <div class="max-w-4xl mx-auto">
        <div class="flex justify-center">
            <video src="media/stream1.mp4" width="3228" height="1324" controls autoplay muted loop></video>
        </div>
        <p class="text-center mt-4 text-lg font-semibold">Dataset visualization</p>
        <p class="text-center mt-2 text-gray-600">Description.</p>
    </div>
</section>
    <!-- Background Section -->
    <section class="py-8 bg-gray-50">
        <div class="max-w-4xl mx-auto">
            <h2 class="text-2xl font-bold">Background</h2>
            <p class="mt-4 text-gray-700">
               Referential Communication is a specific mode of communication that often occurs within a situated dialogue. This can include identifying, describing, or giving instructions related to objects, locations, or people. This form of communication bridges the perceptual and conceptual understanding of one's surroundings. This relies on multimodal expressions, including spatial language and non-verbal behaviors like gaze and pointing gestures. When discussing spatial contexts, pointing or gesturing becomes a crucial addition to spatial language, providing a more immediate and often clearer method of specifying locations or directing attention to particular objects or areas. For agents to effectively understand and participate in referential communication within a situated dialogue, they need to be capable of interpreting and generating both verbal spatial references and non-verbal cues such as pointing gestures and gaze. This dual capability allows for a more nuanced and efficient exchange of information. 
            </p>
        </div>
    </section>

        <!-- Hugging Face Space Embed Section -->
        <section class="py-8 bg-white">
            <div class="max-w-4xl mx-auto">
                <h2 class="text-2xl font-bold text-center">Try it on Hugging Face Spaces</h2>
                <div class="mt-4 flex justify-center">
                <iframe
            	src="https://annadeichler-mm-conv.static.hf.space"
            	frameborder="0"
            	width="850"
            	height="450"
                ></iframe>

                </div>
            </div>
        </section>
</body>

</html>
